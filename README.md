ğŸ“š AskPy RAG â€” Chat with Your Python PDFs!
Build your own offline RAG (Retrieval-Augmented Generation) app to chat with your personal books and tutorials! ğŸš€

ğŸ›  Tech Stack

Part	Technology
Frontend	Streamlit
Embedding Model	HuggingFace sentence-transformers/all-MiniLM-L6-v2
Vector Database	ChromaDB
LLM (Local Model)	Ollama (e.g., Llama3, Mistral, Phi3)
Framework	LangChain
Environment	Python 3.12 + venv
ğŸ—ï¸ Project Structure
bash
Copy
Edit
askpy-rag/
â”‚
â”œâ”€â”€ app.py                  # Main Streamlit app (chat interface)
â”œâ”€â”€ notebooks/               # Helper notebooks
â”‚    â”œâ”€â”€ 01_pdf_to_db.ipynb     # Process PDFs â” Chunks â” Embeddings â” Vector DB
â”‚    â””â”€â”€ 02_chat_with_rag.ipynb # Optional: manual chat testing notebook
â”‚
â”œâ”€â”€ data/                    # Data folder
â”‚    â””â”€â”€ tutorials/           # (Important!) Place your PDFs here
â”‚
â”œâ”€â”€ db/                      # Chroma vector DB (auto-created after embedding)
â”‚
â”œâ”€â”€ .env                     # Your API keys and environment configs
â”œâ”€â”€ .gitignore               # Exclude venv/, db/, etc.
â”œâ”€â”€ requirements.txt         # Project dependencies
â””â”€â”€ README.md                # This file
ğŸš€ How to Run the Project
1. Install Ollama
Download and install from: ollama.ai

Then pull a local LLM model (example: Llama3):

bash
Copy
Edit
ollama pull llama3
(You can also pull lighter models like phi3, mistral, etc.)

2. Set up the Environment
bash
Copy
Edit
python -m venv venv
# activate venv
# (Linux/macOS)
source venv/bin/activate
# (Windows)
venv\Scripts\activate

# install Python libraries
pip install -r requirements.txt
3. Prepare the Vector Database
âš¡ Important Step:

Create a data/tutorials/ folder manually inside your project (if not already created).

Upload your PDFs into the data/tutorials/ folder.

bash
Copy
Edit
data/
â””â”€â”€ tutorials/
     â”œâ”€â”€ python_book.pdf
     â”œâ”€â”€ python_advanced_guide.pdf
     â””â”€â”€ ...
Then:

Run the notebook notebooks/01_pdf_to_db.ipynb

It will:

Extract text

Split into chunks

Embed chunks

Save into ChromaDB (db/ folder)

4. Launch the Chat App
bash
Copy
Edit
streamlit run app.py
âœ… The app will open at: http://localhost:8501

ğŸ§  How It Works
PDFs â” Split into small readable chunks.

Chunks â” Embedded into dense vectors (meaning-based representations).

Saved into Chroma Vector Database.

When you ask a question:

Query â” Embedded â” Best matching chunks retrieved.

Context â” Sent to Ollama (local LLM) â” Answer generated.

ğŸ’¬ Result: You can "talk" to your books easily!

ğŸ“ˆ Future Improvements
Fine-tune chunk size for better performance

Optimize retrieval settings (k, similarity_score_threshold)

Support multi-document answers

Improve UI/UX with chat history and reset button

Deploy on private server / LAN network

ğŸ“œ License
This project is licensed under the MIT License.
Feel free to use, modify, and build upon it! â¤ï¸

ğŸš€ Ready to Build Your Own Local RAG Assistant?
Let's do it together! ğŸŒŸ

â­ï¸ Star the Repo If You Like It!